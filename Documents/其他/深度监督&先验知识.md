## 第一部分 深度监督

### 概述

- 所谓**深度监督(Deep Supervision)**，就是在深度神经网络的某些**中间隐藏层**加了一个**辅助的分类器**作为一种网络分支来对主干网络进行监督的技巧，用来解决深度神经网络**训练梯度消失**和**收敛速度过慢**等问题。

- 最早在2014年，有一篇主题为DSN(Deeply-Supervised Nets)[1]的论文提出。2015年的一篇Training Deeper Convolutional Networks with Deep Supervision[2]的论文尝试了在更深层结构的网络中使用深度监督技巧。

### 结构

- 通常而言，增加神经网络的深度可以一定程度上提高网络的表征能力，但随着深度加深，会逐渐出现**神经网络难以训练**的情况，其中就包括像**梯度消失**和**梯度爆炸**等现象。为了更好的训练深度网络，我们可以尝试给神经网络的某些层添加一些**辅助的分支分类器**来解决这个问题。这种辅助的分支分类器能够起到一种**判断隐藏层特征图质量好坏**的作用。

- 论文[2]作者根据一些经验法则和实验给出了结论。作者先是把深监督放在网络最后一层，然后跑10-50次迭代，绘制出**中间层的平均梯度值**。然后作者将监督分支添加在**平均梯度消失**(原文中平均梯度小于10e-7)的那一层。随迭代次数变化的各卷积层的平均梯度值如下图所示。在他们的8层网络模型中，第4个卷积层的梯度开始消失，因此将辅助监督分类器加在这一层之后。

![](TyporaImg\梯度消失.jpg)

- 带有深度监督的8层和13层网络结构，以及各个模块含义，如下图所示。

![](TyporaImg\8-layer.png)

![](TyporaImg\13-layer.png)

![](TyporaImg\meaning.png)

- 可以看到，图中在第四个卷积块之后添加了一个监督分类器作为分支。Conv4输出的特征图除了随着主网络进入Conv5之外，也作为输入进入了分支分类器。如图所示，该分支分类器包括一个卷积块、两个带有Dropout和ReLu的全连接块和一个纯全连接块。

### 损失函数

以$\mathcal{W}$和$\mathcal{W_s}$表示主干网络和深度监督分支的权重。

$$
\mathcal{W}=(W_1,\dots,W_11),\\\mathcal{W_s}=(W_{s5},\dots,W_{s8}).
$$
输出层softmax表示为：
$$
p_k=\frac{exp(X_{11(k)})}{\begin{matrix}\sum_k exp(X_{11(k)})\end{matrix}}
$$
主干网络的损失函数为：
$$
\mathcal{L}_0(\mathcal{W})=-\sum_{k=1}^K\space y_klnp_k
$$
深度监督分支的softmax输出为：
$$
p_{sk}=\frac{exp(S_{8(k)})}{\begin{matrix}\sum_k exp(S_{8(k)})\end{matrix}}
$$
深度监督分支的损失函数为：
$$
\mathcal{L}_s(\mathcal{W},\mathcal{W_s})=-\sum_{k=1}^K\space y_klnp_{sk}
$$
可以看到深监督分支的损失函数取决于$\mathcal{W}$，而不是$\mathcal{W_s}$，因为分支结构中倒数第二层的$S_{8}$特征图关联到主干网络的卷积权重$W_1\sim W_4$。

所以，联合损失函数可以表示为：
$$
\mathcal{L}(\mathcal{W},\mathcal{W_s})=\mathcal{L}_0(\mathcal{W})+\alpha_t\mathcal{L}_s(\mathcal{W},\mathcal{W_s})
$$
其中$\alpha_t$可以看作随训练轮数衰减的一个值：
$$
\alpha_t\leftarrow\alpha_t*(1-t/N)
$$

### Torch示例

下面以Torch为例实现一个带深度监督的卷积模块。先定义卷积块：

```python
import torch.nn as nn
# 定义卷积块
# 包含3x3卷积+BN+relu
def conv3x3_bn_relu(in_planes, out_planes, stride=1):
    "3x3 convolution + BN + relu"
    return nn.Sequential(
            nn.Conv2d(in_planes, out_planes, kernel_size=3,
                      stride=stride, padding=1, bias=False),
            BatchNorm2d(out_planes),
            nn.ReLU(inplace=True),
            )
```

带有深监督的卷积模块如下：

```python
class C1DeepSup(nn.Module):
    def __init__(self, num_class=150, fc_dim=2048, use_softmax=False):
        super(C1DeepSup, self).__init__()
        self.use_softmax = use_softmax
        self.cbr = conv3x3_bn_relu(fc_dim, fc_dim // 4, 1)
        self.cbr_deepsup = conv3x3_bn_relu(fc_dim // 2, fc_dim // 4, 1)
        # 最后一层卷积
        self.conv_last = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)
        self.conv_last_deepsup = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)
 
    # 前向计算流程
    def forward(self, conv_out, segSize=None):
        conv5 = conv_out[-1]
        x = self.cbr(conv5)
        x = self.conv_last(x)
        if self.use_softmax:  # is True during inference
            x = nn.functional.interpolate(
                x, size=segSize, mode='bilinear', align_corners=False)
            x = nn.functional.softmax(x, dim=1)
            return x
        # 深监督模块
        conv4 = conv_out[-2]
        _ = self.cbr_deepsup(conv4)
        _ = self.conv_last_deepsup(_)
        
        # 主干卷积网络softmax输出
        x = nn.functional.log_softmax(x, dim=1)
        # 深监督分支网络softmax输出
        _ = nn.functional.log_softmax(_, dim=1)
        return (x, _)
```

#### 参考文献

[1] Lee C Y ,  Xie S ,  Gallagher P , et al. Deeply-Supervised Nets[J]. Eprint Arxiv, 2014:562-570.

[2] Wang L ,  Lee C Y ,  Tu Z , et al. Training Deeper Convolutional Networks with Deep Supervision[J]. Computer Science, 2015.

------

## 第二部分 先验知识

